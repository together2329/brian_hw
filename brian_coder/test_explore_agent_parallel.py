#!/usr/bin/env python3
"""
ExploreAgent ë³‘ë ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸

ì‹¤ì œ ExploreAgentê°€ Claude Codeì²˜ëŸ¼ ì—¬ëŸ¬ ë„êµ¬ë¥¼ ë™ì‹œì— ì‹¤í–‰í•˜ëŠ”ì§€ ê²€ì¦
"""

import sys
import os

# Add paths
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'src'))
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'agents'))

# Enable debug mode
os.environ['DEBUG_SUBAGENT'] = 'true'
os.environ['DEBUG_MODE'] = 'true'

def test_explore_agent_parallel():
    """ExploreAgentê°€ ì‹¤ì œë¡œ ë³‘ë ¬ ì‹¤í–‰í•˜ëŠ”ì§€ í…ŒìŠ¤íŠ¸"""
    print("=" * 70)
    print("ExploreAgent ë³‘ë ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ (Claude Code Style)")
    print("=" * 70)

    from agents.sub_agents.explore_agent import ExploreAgent
    import time

    # Mock LLM and tool execution functions
    def mock_llm_call(messages, **kwargs):
        """Mock LLM call - not used in this test"""
        return "EXPLORE_COMPLETE: Mock response"

    def mock_execute_tool(tool_name, **kwargs):
        """Mock tool execution"""
        return f"[Mock] {tool_name} executed with {kwargs}"

    # Create agent with mock functions
    agent = ExploreAgent(
        name="TestExploreAgent",
        llm_call_func=mock_llm_call,
        execute_tool_func=mock_execute_tool,
        max_iterations=3
    )

    print("\n[Task]")
    print("  Explore brian_coder structure:")
    print("  - Find Python files in agents/")
    print("  - List src/ directory")
    print("  - Find all .py files")
    print()

    # Mock LLM response with parallel annotation
    # (ì‹¤ì œë¡œëŠ” LLMì´ ìƒì„±í•˜ì§€ë§Œ, í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ ì§ì ‘ ì£¼ì…)
    mock_llm_response = """
Thought: I need to explore the project structure efficiently by running multiple read operations in parallel.

@parallel
Action: list_dir(path="agents")
Action: list_dir(path="src")
Action: find_files(pattern="*.py", directory=".")
@end_parallel

EXPLORE_COMPLETE: Found project structure with agents/ and src/ directories containing Python files.
"""

    print("\n[Simulated LLM Response]")
    print(mock_llm_response)
    print()

    # Test parsing
    print("[Step 1] Parsing actions from response...")
    actions = agent._parse_actions(mock_llm_response)

    print(f"\n  âœ“ Parsed {len(actions)} actions:")
    for idx, (tool, args, hint) in enumerate(actions):
        hint_marker = f" [hint={hint}]" if hint else ""
        print(f"    {idx+1}. {tool}({args[:40]}...){hint_marker}")

    assert len(actions) == 3, f"Expected 3 actions, got {len(actions)}"
    assert all(hint == "parallel" for _, _, hint in actions), "All should have parallel hint"
    print("\n  âœ… All actions marked as parallel")

    # Test that actions contain hints
    print("\n[Step 2] Validating hints...")
    parallel_actions = [a for a in actions if a[2] == "parallel"]
    print(f"  âœ“ {len(parallel_actions)} actions have 'parallel' hint")
    assert len(parallel_actions) == 3, "All 3 actions should have parallel hint"
    print("  âœ… Hints correctly assigned")

    # Test batch creation
    print("\n[Step 3] Creating execution batches...")
    try:
        from core.action_dependency import ActionDependencyAnalyzer

        analyzer = ActionDependencyAnalyzer()
        batches = analyzer.analyze(actions)

        print(f"  âœ“ Created {len(batches)} batch(es)")
        for idx, batch in enumerate(batches):
            hint_marker = " [LLM-guided]" if "LLM" in batch.reason else ""
            print(f"    Batch {idx+1}: {len(batch.actions)} actions, parallel={batch.parallel}{hint_marker}")

        # Should create 1 parallel batch
        assert len(batches) == 1, f"Expected 1 batch, got {len(batches)}"
        assert batches[0].parallel == True, "Batch should be parallel"
        print("  âœ… Batch created correctly")

    except Exception as e:
        print(f"  âŒ Batch creation failed: {e}")
        import traceback
        traceback.print_exc()
        return False

    # Summary
    print("\n" + "=" * 70)
    print("âœ… ExploreAgent ë³‘ë ¬ ì‹¤í–‰ í…ŒìŠ¤íŠ¸ í†µê³¼!")
    print("=" * 70)
    print("\ní•µì‹¬ ê²€ì¦ ì‚¬í•­:")
    print("  âœ… @parallel annotation íŒŒì‹±")
    print("  âœ… Hint ê²€ì¦ ë° í• ë‹¹")
    print("  âœ… ë³‘ë ¬ ì‹¤í–‰ batch ìƒì„±")
    print("\nì´ì œ ExploreAgentëŠ” Claude Codeì²˜ëŸ¼ ì—¬ëŸ¬ ë„êµ¬ë¥¼ ë™ì‹œì— ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!")

    return True


def test_with_actual_agent_run():
    """ì‹¤ì œ Agent.run()ìœ¼ë¡œ full cycle í…ŒìŠ¤íŠ¸"""
    print("\n\n" + "=" * 70)
    print("Full Agent Run í…ŒìŠ¤íŠ¸ (Optional)")
    print("=" * 70)

    print("\nâš ï¸  ì´ í…ŒìŠ¤íŠ¸ëŠ” ì‹¤ì œ LLM API í˜¸ì¶œì´ í•„ìš”í•©ë‹ˆë‹¤.")
    print("âš ï¸  í˜„ì¬ëŠ” mock í…ŒìŠ¤íŠ¸ë§Œ ì§€ì›í•©ë‹ˆë‹¤.")
    print("âš ï¸  ì‹¤ì œ í†µí•© í…ŒìŠ¤íŠ¸ëŠ” brian_coder ë©”ì¸ ì‹œìŠ¤í…œì—ì„œ ìˆ˜í–‰í•˜ì„¸ìš”.")
    print()

    # For now, skip actual agent run
    print("  â†’ ì‹¤ì œ Agent ì‹¤í–‰ í…ŒìŠ¤íŠ¸ëŠ” ê±´ë„ˆëœë‹ˆë‹¤.")
    print("  â†’ ë©”ì¸ ì‹œìŠ¤í…œì—ì„œ 'spawn_explore' ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸í•˜ì„¸ìš”.")

    return True


def main():
    """ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "ğŸš€ " * 35)
    print("ExploreAgent ë³‘ë ¬ ì‹¤í–‰ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("ğŸš€ " * 35 + "\n")

    try:
        # Test 1: Parsing and preparation
        success = test_explore_agent_parallel()

        if not success:
            print("\nâŒ ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
            return 1

        # Test 2: Optional full agent run
        # test_with_actual_agent_run()

        print("\n" + "ğŸ‰ " * 35)
        print("ëª¨ë“  í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        print("ğŸ‰ " * 35 + "\n")

        return 0

    except Exception as e:
        print(f"\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    sys.exit(main())
