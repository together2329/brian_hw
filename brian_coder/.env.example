# Brian Coder Environment Variables
# Copy this file to .env and fill in your API keys

# ============================================================
# OpenAI Configuration
# ============================================================
LLM_BASE_URL=https://api.openai.com/v1
LLM_API_KEY=your-openai-api-key-here
LLM_MODEL_NAME=gpt-4o-mini

# ============================================================
# OpenRouter Configuration (Uncomment to use)
# ============================================================
# LLM_BASE_URL=https://openrouter.ai/api/v1
# LLM_API_KEY=your-openrouter-api-key-here
# LLM_MODEL_NAME=meta-llama/llama-3.3-70b-instruct:free

# ============================================================
# Brian Coder Settings
# ============================================================
# Rate limiting (seconds between API calls, 0 to disable)
RATE_LIMIT_DELAY=5

# Maximum ReAct loop iterations
MAX_ITERATIONS=5

# Save conversation history
SAVE_HISTORY=true
HISTORY_FILE=conversation_history.json

# Debug mode - show detailed parsing and execution info
# Set to true to see DEBUG messages for troubleshooting
DEBUG_MODE=false

# Tool result preview settings
# How many lines to show for read_file/read_lines results
TOOL_RESULT_PREVIEW_LINES=3
# How many characters to show for other tool results
TOOL_RESULT_PREVIEW_CHARS=300

# Context Management
MAX_CONTEXT_CHARS=262144
COMPRESSION_THRESHOLD=0.8
ENABLE_COMPRESSION=true

# ============================================================
# Prompt Caching Configuration (Anthropic Claude)
# ============================================================
# Enable Anthropic Prompt Caching (manual control)
# Set to true only when using Anthropic Claude models
# Cost savings: 90% for cached tokens (System prompt + repeated context)
# Speed improvement: 20x faster prefill for cached content
ENABLE_PROMPT_CACHING=false

# Maximum cache breakpoints (1-4, Anthropic allows up to 4)
# Default: 3 (System message + 2 dynamic points in history)
# Higher values cache more context but may hit Anthropic limits
MAX_CACHE_BREAKPOINTS=3

# Cache interval - how often to place breakpoints in message history
# 0 = dynamic calculation based on history length (recommended)
# N = place breakpoint every N messages (e.g., 10 = every 10th message)
# Examples:
#   0 = dynamic (adapts to conversation length)
#   5 = cache every 5 messages
#   10 = cache every 10 messages
CACHE_INTERVAL=0

# Minimum tokens required for a message to be cached
# Claude Sonnet/Opus: 1024 tokens minimum
# Claude Haiku: 2048 tokens minimum
# Set higher to cache only substantial messages
MIN_CACHE_TOKENS=1024
