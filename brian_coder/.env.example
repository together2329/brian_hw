# Brian Coder Environment Variables
# Copy this file to .env and fill in your API keys

# ============================================================
# OpenAI Configuration
# ============================================================
LLM_BASE_URL=https://api.openai.com/v1
LLM_API_KEY=your-openai-api-key-here
LLM_MODEL_NAME=gpt-4o-mini

# ============================================================
# OpenRouter Configuration (Uncomment to use)
# ============================================================
# LLM_BASE_URL=https://openrouter.ai/api/v1
# LLM_API_KEY=your-openrouter-api-key-here
# LLM_MODEL_NAME=meta-llama/llama-3.3-70b-instruct:free

# ============================================================
# Brian Coder Settings
# ============================================================
# Rate limiting (seconds between API calls, 0 to disable)
RATE_LIMIT_DELAY=5

# Maximum ReAct loop iterations
MAX_ITERATIONS=100

# API timeout in seconds (how long to wait for API response)
# Set to 0 to disable timeout (not recommended)
API_TIMEOUT=60

# Save conversation history
SAVE_HISTORY=true
HISTORY_FILE=conversation_history.json

# Debug mode - show detailed parsing and execution info
# Set to true to see DEBUG messages for troubleshooting
# When enabled, shows context usage before every iteration
# When disabled, shows context usage only at start and first iteration
DEBUG_MODE=false

# Tool result preview settings
# How many lines to show for read_file/read_lines results
TOOL_RESULT_PREVIEW_LINES=50
# How many characters to show for other tool results
TOOL_RESULT_PREVIEW_CHARS=2000

# Large File Handling
# Maximum characters to show in observation before truncation (default: 20000)
MAX_OBSERVATION_CHARS=20000
# Number of lines to show in preview for large files (default: 100)
LARGE_FILE_PREVIEW_LINES=100

# Context Management (Token-Based)
# Maximum context size in characters (1 token ≈ 4 chars)
# Default: 262144 chars = ~65K tokens (matches Claude's 200K context window)
MAX_CONTEXT_CHARS=262144

# Compression threshold (0.0 to 1.0)
# Triggers compression when context exceeds this percentage of MAX_CONTEXT_CHARS
# Default: 0.8 (80% = ~52K tokens)
COMPRESSION_THRESHOLD=0.8

# Enable automatic context compression
# When enabled, summarizes old conversation history to stay within token limits
ENABLE_COMPRESSION=true

# Compression mode: 'single' or 'chunked'
# single: Summarize all old messages at once (default, faster, cheaper)
# chunked: Summarize in chunks like Strix (better for very long histories, more expensive)
COMPRESSION_MODE=single

# Chunk size for chunked compression (only used when COMPRESSION_MODE=chunked)
# How many messages to summarize per chunk
# Default: 10 (like Strix)
COMPRESSION_CHUNK_SIZE=10

# Number of recent messages to keep unchanged during compression
# These messages are preserved as-is without summarization
# Default: 4, Range: 4-15 (Strix uses 15)
COMPRESSION_KEEP_RECENT=4

# Enable Smart Compression (selective preservation based on importance)
# When enabled, preserves critical messages (user preferences, error solutions)
# and only summarizes less important messages (exploration, failed attempts)
# Default: true
ENABLE_SMART_COMPRESSION=true

# Context Usage Display:
# Brian Coder now shows real-time context usage with visual progress bar:
#   [Context: 1,979/65,536 tokens (3%) ░░░░░░░░░░░░░░░░░░░░ OK]
#   - Green: 0-49% (OK)
#   - Cyan: 50-79% (MEDIUM)
#   - Yellow: 80-99% (HIGH) - compression will trigger soon
#   - Red: 100%+ (OVER LIMIT)

# ============================================================
# Prompt Caching Configuration (Anthropic Claude)
# ============================================================
# Enable Anthropic Prompt Caching (manual control)
# Set to true only when using Anthropic Claude models
# Cost savings: 90% for cached tokens (System prompt + repeated context)
# Speed improvement: 20x faster prefill for cached content
ENABLE_PROMPT_CACHING=false

# Maximum cache breakpoints (1-4, Anthropic allows up to 4)
# Default: 3 (System message + 2 dynamic points in history)
# Higher values cache more context but may hit Anthropic limits
MAX_CACHE_BREAKPOINTS=3

# Cache interval - how often to place breakpoints in message history
# 0 = dynamic calculation based on history length (recommended)
# N = place breakpoint every N messages (e.g., 10 = every 10th message)
# Examples:
#   0 = dynamic (adapts to conversation length)
#   5 = cache every 5 messages
#   10 = cache every 10 messages
CACHE_INTERVAL=0

# Minimum tokens required for a message to be cached
# Claude Sonnet/Opus: 1024 tokens minimum
# Claude Haiku: 2048 tokens minimum
# Set higher to cache only substantial messages
MIN_CACHE_TOKENS=1024

# ============================================================
# Prompt Caching Usage Example
# ============================================================
# When enabled with DEBUG_MODE=true, you'll see:
#   [System] Cache breakpoint 1/3: System message (2022 tokens)
#   [System] Cache interval: 10 messages
#   [System] Total cache breakpoints applied: 2/3
#
# After each API response, token usage will be displayed:
#   [Token Usage]
#     Input: 550 tokens
#     Output: 200 tokens
#     Cache Hit: 2,000 tokens (saved ~1,800 tokens worth of cost!)
#
# Cost savings example (100 conversations):
#   Without caching: 100 × 2,500 tokens = 250,000 tokens = $0.75
#   With caching: 2,500 + (99 × 550) = 57,000 tokens = $0.17
#   Total savings: $0.58 (77% reduction)

# ============================================================
# Embedding Configuration (for Memory System)
# ============================================================
# Embedding API URL (OpenAI compatible endpoint)
EMBEDDING_BASE_URL=https://api.openai.com/v1

# Embedding API Key (can be same as LLM_API_KEY or different)
# If not set, will use LLM_API_KEY
EMBEDDING_API_KEY=

# Embedding model name
# OpenAI: text-embedding-3-small (1536 dimensions, $0.00002/1K tokens)
#         text-embedding-3-large (3072 dimensions, $0.00013/1K tokens)
# For OpenRouter or other providers, check their documentation
EMBEDDING_MODEL=text-embedding-3-small

# Embedding dimension (auto-detected from model, optional override)
# text-embedding-3-small: 1536
# text-embedding-3-large: 3072
EMBEDDING_DIMENSION=1536

# ============================================================
# Memory System Configuration
# ============================================================
# Enable memory system (stores user preferences and project context)
# Default: true
ENABLE_MEMORY=true

# Memory directory (relative to home directory)
# Files will be stored in ~/.brian_memory/
# Default: .brian_memory
MEMORY_DIR=.brian_memory

# Enable automatic preference extraction from user messages (Mem0-style)
# When enabled, Brian Coder automatically detects and learns preferences
# Example: "From now on, use camelCase" → auto-saves to memory
# Includes Add/Update/Delete logic with LLM-based conflict resolution
# Default: true
ENABLE_AUTO_EXTRACT=true

# ============================================================
# Graph Lite Configuration (Knowledge Graph)
# ============================================================
# Enable Graph Lite system (stores entities, relations, and semantic knowledge)
# Provides semantic search and knowledge extraction from conversations
# Default: true
ENABLE_GRAPH=true

# Auto-extract knowledge at end of conversations
# When enabled, automatically extracts entities and relations from conversation
# and stores them in the knowledge graph for future reference
# Default: true
GRAPH_AUTO_EXTRACT=true

# Number of relevant knowledge graph nodes to inject into context
# Uses semantic search to find most relevant nodes based on current conversation
# Higher values provide more context but use more tokens
# Default: 5, Range: 3-10
GRAPH_SEARCH_LIMIT=5

# Similarity threshold for graph search results (0.0-1.0)
# Only nodes with similarity >= this value will be included in context
# Default: 0.5, Range: 0.3-0.8
GRAPH_SIMILARITY_THRESHOLD=0.5

# Number of recent messages to use for knowledge extraction at end of session
# Default: 10, Range: 5-20
GRAPH_EXTRACTION_MESSAGES=10

# ============================================================
# A-MEM Configuration (Auto-Linking Knowledge Graph)
# ============================================================
# Similarity threshold for finding candidate notes to link (0.0-1.0)
# When adding a new note, candidates with embedding similarity >= threshold
# are sent to LLM for linking decision
# Default: 0.5, Range: 0.3-0.7
AMEM_SIMILARITY_THRESHOLD=0.5

# Maximum number of candidate notes to send to LLM for linking decision
# Higher values = more thorough linking but more expensive LLM calls
# Default: 10, Range: 5-20
AMEM_MAX_CANDIDATES=10

# LLM temperature for auto-linking decisions (0.0-1.0)
# Lower = more logical/deterministic, Higher = more creative/exploratory
# Default: 0.3 (logical linking)
AMEM_LINK_TEMPERATURE=0.3

# ============================================================
# Procedural Memory Configuration (Memp - Learning from Experience)
# ============================================================
# Enable Procedural Memory system (learns from past task experiences)
# Stores how-to knowledge: task → actions → outcome
# Helps avoid repeating same mistakes and follow proven approaches
# Default: true
ENABLE_PROCEDURAL_MEMORY=true

# Maximum number of similar past trajectories to retrieve for guidance
# When starting a new task, Brian Coder retrieves similar past experiences
# Default: 3, Range: 1-5
PROCEDURAL_RETRIEVE_LIMIT=3

# Minimum similarity score to use a trajectory for guidance (0.0-1.0)
# Only trajectories with similarity >= threshold will be shown
# Lower = more guidance, Higher = only very similar tasks
# Default: 0.5, Range: 0.3-0.8
PROCEDURAL_SIMILARITY_THRESHOLD=0.5

# Inject past experience guidance into prompts
# When enabled, shows successful approaches from similar past tasks
# Default: true
PROCEDURAL_INJECT_GUIDANCE=true

# ============================================================
# Deep Think Configuration (Hypothesis Branching)
# ============================================================
# Enable Deep Think system (parallel hypothesis reasoning)
# When enabled, Brian Coder generates multiple approaches before executing
# Pipeline: Branching → Simulation → Scoring → Selection
# Default: false (opt-in feature)
ENABLE_DEEP_THINK=false

# Number of hypotheses to generate per task
# More hypotheses = more diverse approaches but higher latency/cost
# Default: 3, Range: 2-5
DEEP_THINK_NUM_HYPOTHESES=3

# Enable simulation step (actually run first_action for each hypothesis)
# When enabled, executes read-only tools to validate approaches
# Default: true
DEEP_THINK_ENABLE_SIMULATION=true

# Scoring weights (must sum to 1.0)
# Adjust to prioritize different evaluation dimensions
DEEP_THINK_WEIGHT_EXPERIENCE=0.30    # Past success rate from ProceduralMemory
DEEP_THINK_WEIGHT_KNOWLEDGE=0.20     # Related nodes from GraphLite
DEEP_THINK_WEIGHT_COHERENCE=0.25     # LLM-judged logical consistency
DEEP_THINK_WEIGHT_SIMULATION=0.15    # Simulation result quality
DEEP_THINK_WEIGHT_CONFIDENCE=0.10    # Initial confidence boost

# LLM temperature for hypothesis generation
# Higher = more diverse/creative approaches
# Default: 0.7, Range: 0.5-1.0
DEEP_THINK_TEMPERATURE=0.7

# Timeout for parallel tool execution (seconds)
# Each hypothesis simulation is limited to this time
# Default: 10, Range: 5-30
DEEP_THINK_TOOL_TIMEOUT=10

# ============================================================
# Verilog Analysis Tools Plugin
# ============================================================
# Enable Verilog-specific analysis tools (optional plugin)
# Adds 10 specialized tools for Verilog development:
#   - analyze_verilog_module: Parse module structure
#   - find_signal_usage: Track signal across files
#   - find_module_definition: Locate module files
#   - extract_module_hierarchy: Visualize project structure
#   - generate_module_testbench: Auto-generate testbenches
#   - generate_waveform_dict: GTKWave script generation
#   - generate_module_docs: Markdown documentation
#   - find_potential_issues: Static analysis (linting)
#   - analyze_timing_paths: Logic depth estimation
#   - suggest_optimizations: Resource/FSM optimization tips
# Default: false (enabled when working with Verilog projects)
ENABLE_VERILOG_TOOLS=false
