# Brian Coder Environment Variables
# Copy this file to .env and fill in your API keys

# ============================================================
# OpenAI Configuration
# ============================================================
LLM_BASE_URL=https://api.openai.com/v1
LLM_API_KEY=your-openai-api-key-here
LLM_MODEL_NAME=gpt-4o-mini

# ============================================================
# OpenRouter Configuration (Uncomment to use)
# ============================================================
# LLM_BASE_URL=https://openrouter.ai/api/v1
# LLM_API_KEY=your-openrouter-api-key-here
# LLM_MODEL_NAME=meta-llama/llama-3.3-70b-instruct:free

# ============================================================
# Brian Coder Settings
# ============================================================
# Rate limiting (seconds between API calls, 0 to disable)
RATE_LIMIT_DELAY=5

# Maximum ReAct loop iterations
MAX_ITERATIONS=5

# Save conversation history
SAVE_HISTORY=true
HISTORY_FILE=conversation_history.json

# Debug mode - show detailed parsing and execution info
# Set to true to see DEBUG messages for troubleshooting
# When enabled, shows context usage before every iteration
# When disabled, shows context usage only at start and first iteration
DEBUG_MODE=false

# Tool result preview settings
# How many lines to show for read_file/read_lines results
TOOL_RESULT_PREVIEW_LINES=3
# How many characters to show for other tool results
TOOL_RESULT_PREVIEW_CHARS=300
42: 
43: # Large File Handling
44: # Maximum characters to show in observation before truncation (default: 20000)
45: MAX_OBSERVATION_CHARS=20000
46: # Number of lines to show in preview for large files (default: 100)
47: LARGE_FILE_PREVIEW_LINES=100

# Context Management (Token-Based)
# Maximum context size in characters (1 token ≈ 4 chars)
# Default: 262144 chars = ~65K tokens (matches Claude's 200K context window)
MAX_CONTEXT_CHARS=262144

# Compression threshold (0.0 to 1.0)
# Triggers compression when context exceeds this percentage of MAX_CONTEXT_CHARS
# Default: 0.8 (80% = ~52K tokens)
COMPRESSION_THRESHOLD=0.8

# Enable automatic context compression
# When enabled, summarizes old conversation history to stay within token limits
ENABLE_COMPRESSION=true

# Compression mode: 'single' or 'chunked'
# single: Summarize all old messages at once (default, faster, cheaper)
# chunked: Summarize in chunks like Strix (better for very long histories, more expensive)
COMPRESSION_MODE=single

# Chunk size for chunked compression (only used when COMPRESSION_MODE=chunked)
# How many messages to summarize per chunk
# Default: 10 (like Strix)
COMPRESSION_CHUNK_SIZE=10

# Number of recent messages to keep unchanged during compression
# These messages are preserved as-is without summarization
# Default: 4, Range: 4-15 (Strix uses 15)
COMPRESSION_KEEP_RECENT=4

# Context Usage Display:
# Brian Coder now shows real-time context usage with visual progress bar:
#   [Context: 1,979/65,536 tokens (3%) ░░░░░░░░░░░░░░░░░░░░ OK]
#   - Green: 0-49% (OK)
#   - Cyan: 50-79% (MEDIUM)
#   - Yellow: 80-99% (HIGH) - compression will trigger soon
#   - Red: 100%+ (OVER LIMIT)

# ============================================================
# Prompt Caching Configuration (Anthropic Claude)
# ============================================================
# Enable Anthropic Prompt Caching (manual control)
# Set to true only when using Anthropic Claude models
# Cost savings: 90% for cached tokens (System prompt + repeated context)
# Speed improvement: 20x faster prefill for cached content
ENABLE_PROMPT_CACHING=false

# Maximum cache breakpoints (1-4, Anthropic allows up to 4)
# Default: 3 (System message + 2 dynamic points in history)
# Higher values cache more context but may hit Anthropic limits
MAX_CACHE_BREAKPOINTS=3

# Cache interval - how often to place breakpoints in message history
# 0 = dynamic calculation based on history length (recommended)
# N = place breakpoint every N messages (e.g., 10 = every 10th message)
# Examples:
#   0 = dynamic (adapts to conversation length)
#   5 = cache every 5 messages
#   10 = cache every 10 messages
CACHE_INTERVAL=0

# Minimum tokens required for a message to be cached
# Claude Sonnet/Opus: 1024 tokens minimum
# Claude Haiku: 2048 tokens minimum
# Set higher to cache only substantial messages
MIN_CACHE_TOKENS=1024

# ============================================================
# Prompt Caching Usage Example
# ============================================================
# When enabled with DEBUG_MODE=true, you'll see:
#   [System] Cache breakpoint 1/3: System message (2022 tokens)
#   [System] Cache interval: 10 messages
#   [System] Total cache breakpoints applied: 2/3
#
# After each API response, token usage will be displayed:
#   [Token Usage]
#     Input: 550 tokens
#     Output: 200 tokens
#     Cache Hit: 2,000 tokens (saved ~1,800 tokens worth of cost!)
#
# Cost savings example (100 conversations):
#   Without caching: 100 × 2,500 tokens = 250,000 tokens = $0.75
#   With caching: 2,500 + (99 × 550) = 57,000 tokens = $0.17
#   Total savings: $0.58 (77% reduction)
